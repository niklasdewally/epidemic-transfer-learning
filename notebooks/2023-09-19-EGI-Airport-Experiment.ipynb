{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f0560c-4cfb-4d44-8e19-7461103c6cbf",
   "metadata": {},
   "source": [
    "# EGI Airport Experiment\n",
    "\n",
    "This notebook contains a minimal reproduction of the airport experiment from the [EGI](https://arxiv.org/abs/2009.05204) paper.\n",
    "\n",
    "The aim of this experiment is to learn node labels from a graph of the airports\n",
    "of one region, and transfer them to another region. This transfer will occur\n",
    "directly, without finetuning.\n",
    "\n",
    "The node labels are the relative popularity of the airports, as quartiles (1-4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8b3cb7-08af-47b1-92b1-4faf30002eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import wandb\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "\n",
    "import gtl\n",
    "import gtl.training\n",
    "from gtl import Graph\n",
    "from gtl.typing import PathLike\n",
    "from gtl.features import degree_bucketing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f11532e-a6d9-4146-a35b-41722e5241b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/niklas/src/staris/main/data/airports\n"
     ]
    }
   ],
   "source": [
    "# where is the data stored?\n",
    "# in this case, i have it committed in the git repo under data/airports\n",
    "# larger datasets should be downloaded seperately!\n",
    "DATA_DIR: Path = Path().cwd().parent / \"data\" / \"airports\"\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6cd3285-1e5b-4590-8d94-5b548ad824dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-detect if we are on a GPU or not\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc28574-6565-4bc5-b10a-baac24f41eef",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "The data contains two files for each region: a list of edges, and a list of labels for each edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bdde761-29f6-4943-9f01-bada32a541a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 36\n",
      "\n",
      "57 50\n",
      "\n",
      "43 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_DIR / \"europe-airports.edgelist\") as f:\n",
    "    for i in range(3):\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78938f5c-26a6-4788-a17f-0b8ec119cd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node label\n",
      "\n",
      "0 1\n",
      "\n",
      "1 1\n",
      "\n",
      "2 2\n",
      "\n",
      "3 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_DIR / \"labels-europe-airports.txt\") as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7ad926-e938-456d-94ca-986b925f39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(edgefile: PathLike, labelfile: PathLike) -> tuple[Graph, NDArray]:\n",
    "    edges = np.loadtxt(edgefile, dtype=\"int\")\n",
    "    us = torch.from_numpy(edges[:, 0]).to(device)\n",
    "    vs = torch.from_numpy(edges[:, 1]).to(device)\n",
    "    dgl_graph: dgl.DGLGraph = dgl.graph((us, vs), device=torch.device(\"cpu\"))\n",
    "    dgl_graph = dgl.to_bidirected(dgl_graph).to(device)\n",
    "\n",
    "    \n",
    "    graph: Graph = gtl.Graph.from_dgl_graph(dgl_graph)\n",
    "    #graph.mine_triangles() # only necessary for triangle model.\n",
    "    \n",
    "    labels = np.loadtxt(labelfile, skiprows=1)\n",
    "    return graph, labels[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d3a709-08fa-4f33-8410-4885f2cf283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "europe_g,europe_labels = load_dataset(DATA_DIR / \"europe-airports.edgelist\",DATA_DIR / \"labels-europe-airports.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67267723-1158-4343-8b47-1b4e6b297a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "brazil_g,brazil_labels = load_dataset(DATA_DIR / \"brazil-airports.edgelist\",DATA_DIR / \"labels-brazil-airports.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83454ec7-c5ee-4517-b251-e06aa41b7c22",
   "metadata": {},
   "source": [
    "## Running the model\n",
    "Now, we define a single run of the model.\n",
    "\n",
    "\n",
    "In this example we will use EGI, transferring from europe to brazil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0a71c3-4ddd-4d4b-9605-84c03b0c3b34",
   "metadata": {},
   "source": [
    "Configuration options (including hyperparamaters) must be defined in a single dictionary.\n",
    "\n",
    "Valid options are listed in the gtl.training.train function documentation. Invalid options are ignored silently, allowing this config dict to be used for other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3fce355-18de-4495-81e8-7a4016971dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mgtl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\n",
       "\u001b[1;33m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\n",
       "\u001b[1;33m\u001b[0m    \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgtl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\n",
       "\u001b[1;33m\u001b[0m    \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\n",
       "\u001b[1;33m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\n",
       "\u001b[1;33m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\n",
       "\u001b[1;33m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheterograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDGLGraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Using `graph` and `features`, train an instance of the given model.\n",
       "\n",
       "The model is trained in an unsupervised manner, producing a set of\n",
       "node-embeddings to be used as input for downstream models like\n",
       "classifier.\n",
       "\n",
       "Args:\n",
       "\n",
       "    model: The model to use.\n",
       "\n",
       "        Implemented models are:\n",
       "            * graphsage-mean\n",
       "            * graphsage-pool\n",
       "            * graphsage-gcn\n",
       "            * graphsage-lstm\n",
       "            * egi\n",
       "            * triangle\n",
       "\n",
       "    graph: The graph to use for training, in gtl format.\n",
       "\n",
       "    features: A tensor containing features for each node in `graph`.\n",
       "\n",
       "    config: A dictionary containing hyperparameters and training settings.\n",
       "        See below for more details.\n",
       "\n",
       "    device: The pytorch device to use for training and inference.\n",
       "        If not specified, a device will automatically be selected.\n",
       "\n",
       "\n",
       "Returns: a trained graph encoder.\n",
       "\n",
       "    This takes in the graph in DGL format, and a feature tensor, and returns\n",
       "    a tensor of node embeddings.\n",
       "\n",
       "\n",
       "Configuration values:\n",
       "\n",
       "    The configuration dictionary takes the following values:\n",
       "\n",
       "    Hyperparameters\n",
       "    ===============\n",
       "        * hidden_layers (REQUIRED)\n",
       "        * k (REQUIRED)\n",
       "        * lr (REQUIRED)\n",
       "        * batch_size (optional)\n",
       "        * n_epochs (optional)\n",
       "\n",
       "\n",
       "    Early Stopping\n",
       "    ==============\n",
       "        * patience (optional)\n",
       "            If patience is not specified, early stopping is disabled.\n",
       "\n",
       "        * min_delta (optional)\n",
       "\n",
       "    Transfer learning\n",
       "    =================\n",
       "        * load_weights_from: a model file, compatible with torch.load() to\n",
       "          load intial weights from.\n",
       "\n",
       "          This must be from the same type and size of model as is specified\n",
       "          in this function call.\n",
       "\n",
       "\n",
       "        * save_weights_to: a path to save the trained model to.\n",
       "\n",
       "    Logging\n",
       "    ========\n",
       "\n",
       "    wandb is used for logging.\n",
       "\n",
       "    * wandb_summary_prefix: a prefix to add to reported losses in wandb\n",
       "        (optional; defaults to \"\").\n",
       "\u001b[1;31mFile:\u001b[0m      ~/src/staris/main/src/gtl/training/__init__.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?gtl.training.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d52afcd-be99-4028-bd28-165b44e8cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"lr\": 0.01,\n",
    "  \"hidden_layers\": 32,\n",
    "  \"patience\": 50,\n",
    "  \"min_delta\": 0.01,\n",
    "  \"n_epochs\": 200,\n",
    "  \"k\":2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f78a4-dff4-4d6e-ad97-ecc947a58673",
   "metadata": {},
   "source": [
    "We have no node features, so we create some using degree bucketing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7178846-f2e3-4e4d-aac3-9281cc26415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node features for encoder\n",
    "europe_node_feats = degree_bucketing(\n",
    "    europe_g.as_dgl_graph(device), config[\"hidden_layers\"]\n",
    ").to(device)\n",
    "brazil_node_feats = degree_bucketing(\n",
    "    brazil_g.as_dgl_graph(device), config[\"hidden_layers\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a485e8d5-d7f9-4576-89b2-bb439d63ef1d",
   "metadata": {},
   "source": [
    "Metrics and results are tracked using Weights and Biases runs. We will use this in offline mode for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "518db672-e949-4cc0-9f8e-25ea821489b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x17852f0d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(mode=\"offline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b64749-57d9-4255-a57f-aaf8a6ab9aa8",
   "metadata": {},
   "source": [
    "We now have everything we need to train the source encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c809d845-c43d-4538-9b15-5e169a8e5370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                 | 0/200 [00:00<?, ?it/s]/Users/niklas/Library/Caches/pypoetry/virtualenvs/graphtransferlearning-TWiZ5Iw6-py3.11/lib/python3.11/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n",
      " 46%|███████████████████████████████████████████████████████▊                                                                | 93/200 [00:25<00:29,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = gtl.training.train(\"egi\",europe_g,europe_node_feats,config,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6b15643-80f0-48b0-939b-d78c5625367d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Accuracy 0.46\n",
      "Target Accuracy 0.6060606060606061\n"
     ]
    }
   ],
   "source": [
    "# Generate graph embeddings\n",
    "source_embs = encoder(europe_g.as_dgl_graph(device),europe_node_feats)\n",
    "\n",
    "# Direct transfer encoder to target\n",
    "target_embs = (\n",
    "    encoder(brazil_g.as_dgl_graph(device), brazil_node_feats)\n",
    "    .to(torch.device(\"cpu\")))\n",
    "\n",
    "# We transfer embeddings, but create seperate node classifiers.\n",
    "# This is up to you.\n",
    "\n",
    "# We use sklearn to create the node classifier.\n",
    "# You could use a MLP using Pytorch instead.\n",
    "train_embs, val_embs, train_classes, val_classes = sklearn.model_selection.train_test_split(\n",
    "    source_embs.detach().numpy(), europe_labels\n",
    ")\n",
    "\n",
    "\n",
    "classifier = sklearn.linear_model.SGDClassifier(loss=\"log_loss\")\n",
    "classifier = classifier.fit(train_embs, train_classes)\n",
    "print(f\"Source Accuracy {classifier.score(val_embs,val_classes)}\")\n",
    "\n",
    "# Now do target accuracy\n",
    "train_embs, val_embs, train_classes, val_classes = sklearn.model_selection.train_test_split(\n",
    "    target_embs.detach().numpy(), brazil_labels\n",
    ")\n",
    "\n",
    "\n",
    "classifier = sklearn.linear_model.SGDClassifier(loss=\"log_loss\")\n",
    "classifier = classifier.fit(train_embs, train_classes)\n",
    "print(f\"Target Accuracy {classifier.score(val_embs,val_classes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6947f799-65ff-425e-9c15-731074cee00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>-training-loss</td><td>██▇▆▅▆▆▅▅▄▃▄▃▃▄▄▄▄▄▄▃▃▃▃▄▂▂▂▂▂▂▂▂▁▃▂▁▁▁▂</td></tr><tr><td>-validation-loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>-early-stopping-epoch</td><td>42</td></tr><tr><td>-training-loss</td><td>-0.22749</td></tr><tr><td>-validation-loss</td><td>-0.09549</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /Users/niklas/src/staris/main/notebooks/wandb/offline-run-20230919_153305-t7zh2knp<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20230919_153305-t7zh2knp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650198a0-8a23-451f-8fd2-c7ad3c313796",
   "metadata": {},
   "source": [
    "## Ideas of things to try to change\n",
    "1. Run this for graphsage's mean and pool variants.\n",
    "2. Turn this code into an experiment that determines the model performance based on different values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a164d00-be51-43cb-8a8a-5a4f1a1bfe69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
